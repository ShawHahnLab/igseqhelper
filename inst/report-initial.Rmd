# Run Performance and Read Depth

```{r}
metadata <- within(list(), {
  runs <- load_runs(file.path(ROOT, "metadata/runs.csv"))
  sequences <- load_sequences(file.path(ROOT, "metadata/sequences.csv"))
  specimens <- load_specimens(file.path(ROOT, "metadata/specimens.csv"))
  antibody_lineages <- load_antibody_lineages(
    file.path(ROOT, "metadata/antibody_lineages.csv"))
  samples <- load_samples(
    file.path(ROOT, "metadata/samples.csv"), specimens, runs, sequences)
  antibody_isolates <- load_antibody_isolates(
    file.path(ROOT, "metadata/antibody_isolates.csv"), antibody_lineages)
})

counts_stats <- within(list(), {
  by_sample <- load_csv_maybe(file.path(ROOT, "analysis/reporting/counts/counts_by_sample.csv"), NA)
  by_sample <- if (!is.null(by_sample)) { subset(by_sample, Sample != "unassigned") }
  by_run <- load_csv_maybe(file.path(ROOT, "analysis/reporting/counts/counts_by_run.csv"))
  by_amplicon <- load_csv_maybe(file.path(ROOT, "analysis/reporting/counts/counts_amplicon_summary.csv"), NA)
  assembly <- load_csv_maybe(file.path(ROOT, "analysis/reporting/counts/counts_assembly_summary.csv"), NA)
  qual <- load_csv_maybe(file.path(ROOT, "analysis/reporting/counts/counts_presto_qual_summary.csv"), NA)
  sonar <- load_csv_maybe(file.path(ROOT, "analysis/reporting/counts/counts_sonar_module1_summary.csv"), NA)
})
```

```{r}
if (! is.null(counts_stats$by_amplicon)) {
  counts_presto <- subset(counts_stats$by_amplicon, select = -Ratio)
  handle <- with(counts_presto, paste(Timepoint, Specimen, Chain, ChainType))
  if (! (length(handle) == length(unique(handle)))) {
    stop("Non-unique IDs for specimen handling")
  }
  if (! is.null(counts_stats$assembly)) {
    idx <- match(handle, with(counts_stats$assembly, paste(Timepoint, Specimen, Chain, ChainType)))
    counts_presto$SeqsAssembled <- counts_stats$assembly$Seqs[idx]
    if (! is.null(counts_stats$qual)) {
     idx <- match(handle, with(counts_stats$qual, paste(Timepoint, Specimen, Chain, ChainType)))
      counts_presto$SeqsQualFilt <- counts_stats$qual$Seqs[idx]
      counts_presto <- counts_presto[, c(colnames(counts_presto)["CellCount" != colnames(counts_presto)], "CellCount")]
      counts_presto$FinalRatio <- with(counts_presto, round(SeqsQualFilt/CellCount, 8))
    }
  }
}
```

## By Sequencing Run

We aim for a 50/50 ratio of library to PhiX. Assuming the unassigned reads are
all PhiX, we expect to see close to 1.0 for the below ratios.

```{r}
drawtab(counts_stats$by_run)
```

## By Sequencing Sample

The libraries are prepared with a goal of roughly ten reads for every one cell,
so we expect to see 10 or more for the below ratios.

```{r}
drawtab(counts_stats$by_sample)
```

## By Sequencing Sample - Barcode Matching

```{r}
load_barcode_summary <- function(csv_fp) {
  # "BCFWD", "BCREV", "BCFWDName", "BCREVName", "Total", "Sample"
  result <- read.csv(
    csv_fp, header = TRUE, stringsAsFactors = FALSE, na.strings = "")
  result$BCFWDName <- factor(
    result$BCFWDName, levels = unique(result$BCFWDName))
  result$BCREVName <- factor(
    result$BCREVName, levels = unique(result$BCREVName))
  ordering <- with(
      result, order(is.na(BCFWDName), is.na(BCREVName), BCFWDName, BCREVName))
  result <- result[ordering, ]
  result
}

# Plot heatmap of read count per barcode combination.  See also
# tabulate_barcodes.
plot_barcodes <- function(barcodes_summary) {
  barcodes_summary$Label <- factor(barcodes_summary$Sample, labels = "S")
  ggplot(
    barcodes_summary,
    aes(x=BCFWDName, y=BCREVName, fill=log10(Total), label=Sample)) +
    geom_tile() +
    geom_label_repel(size = 3, na.rm = TRUE) +
    viridis::scale_fill_viridis() +
    labs(x = "BC Fwd Name", y = "BC Rev Name") +
    theme(
      axis.text.x = element_text(angle=45, hjust=1),
      panel.background = element_blank())
}

dps <- list.files(file.path(ROOT, "analysis/reporting/by-run"), full.names = TRUE)
runids <- basename(dps)
barcodings <- mapply(function(runid, dir_path) {
  csv_fp <- file.path(dir_path, "barcode_summary.csv")
  if (file.exists(csv_fp)) {
    load_barcode_summary(csv_fp)
  }
}, runids, dps, SIMPLIFY = FALSE)
```

```{r fig.width=7, fig.height=4.8}
for(runid in names(barcodings)) {
  if (! is.null(barcodings[[runid]])) {
    plot(plot_barcodes(barcodings[[runid]]) + ggplot2::ggtitle(runid))
    cat("\n\n")
  }
}
```

## By Sequencing Sample - Adapter and Quality Trimming


```{r}
qualgrids_r1 <- list()
qualgrids_r2 <- list()
if (! is.null(counts_stats$by_sample)) {
  for (idx in 1:nrow(counts_stats$by_sample)) {
    prefix <- file.path(
      ROOT,
      "analysis", "reporting", "by-run",
      counts_stats$by_sample$Run[idx],
      paste0("qualtrim.", counts_stats$by_sample$Sample[idx]))
    fp_r1 <- paste0(prefix, ".R1.csv")
    fp_r2 <- paste0(prefix, ".R2.csv")
    if (file.exists(fp_r1)) {
      qualgrids_r1 <- c(qualgrids_r1, list(load_qualgrid(fp_r1)))
    } else {
      qualgrids_r1 <- c(qualgrids_r1, list(NULL))
    }
    if (file.exists(fp_r2)) {
      qualgrids_r2 <- c(qualgrids_r2, list(load_qualgrid(fp_r2)))
    } else {
      qualgrids_r2 <- c(qualgrids_r2, list(NULL))
    }
  }
  names(qualgrids_r1) <- counts_stats$by_sample$Sample
  names(qualgrids_r2) <- counts_stats$by_sample$Sample
}
```

Our read trimming includes both adapter-matching and a quality threshold. 
Different quality thresholds lead to more or less aggressive triming at the
(typically lower-quality) ends of the reads.  Generally speaking, if the quality
scores for a given sample's reads are high, then a varying threshold won't have
much of an effect on the final read length, while if they're low, even a slight
threshold will reduce read length substantially.

I've come up with a value between zero and one for each sample (for both R1 and 
R2) summarizing how much read length tends to change with a varying quality 
threshold.  Values toward zero imply that reads are trimmed substantially with
even a low threshold while values toward one imply that the read length is 
largely unaffected.  The summary scatterplot below shows all samples with the R1
values on the x-axis and R2 values on the y-axis.  Ideally our samples should
stay toward the upper-right; if not, we may have a read quality problem.

```{r fig.width=8, fig.height=6}
qualtrim_metric <- function(qualgrid) {
  if (is.null(qualgrid)) {
    return(NA)
  }
  # At each quality score, what's the median length?  (The math gets goofy
  # because I've already binned the values into the matrix.)
  vec <- apply(qualgrid, 1, function(row) {
    match(TRUE, cumsum(row) > sum(row)/2) })
  # Now, normalizing both axes, what's the integral of the length with respect
  # to the quality?  This will give us a value from zero to one, with lower
  # numbers implying more aggressive trimming for a given quality threshold than
  # higher numbers.
  sum(vec/max(vec))/length(vec)
}

if (! is.null(counts_stats$by_sample)) {
 qualmetrics <- data.frame(
    Sample = names(qualgrids_r1),
    R1Metric = sapply(qualgrids_r1, qualtrim_metric),
    R2Metric = sapply(qualgrids_r2, qualtrim_metric),
    stringsAsFactors = FALSE)

  ggplot(
    qualmetrics,
    aes(x = R1Metric, y = R2Metric, label = Sample)) +
    geom_point() +
    geom_text_repel(size = 2) +
    labs(xlab = "R1 Metric", ylab = "R2 Metric")
}
```

The below heatmaps summarize read lengths after cutadapt's quality-based
trimming with a range of possible quality thresholds.  The Y axis is a given
cutadapt quality threshold, the X axis trimmed read length, and color signifies
log10 count of reads trimmed to that length.  (For very low quality thresholds
all reads are left full-length while for very high thresholds all are trimmed to
zero.)  We can use this to choose a threshold that fits our reads well, for
example by showing a distribution of read lengths that should be biologically
plausible (i.e. some sequences for both chains will be shorter than the
sequencer's configured read length).  This trimming is independent of the
barcode+primer trimming that cutadapt also performs.

```{r fig.width = 3.7, fig.height = 3, dev="png", dpi=150, eval=config$plot_qualgrid}
for (sample in counts_stats$by_sample$Sample) {
  if (! is.null(qualgrids_r1[[sample]]) && ! is.null(qualgrids_r2[[sample]]) && sum(qualgrids_r1[[sample]]) > 100) {
  plot_qualgrid(qualgrids_r1[[sample]], main=paste(sample, "R1"),
                legend = FALSE, show_rownames = FALSE)
  plot_qualgrid(qualgrids_r2[[sample]], main=paste(sample, "R2"))
  cat("\n\n")
  } else {
    cat(paste("\n\nToo few reads for", sample, "\n\n"))
  }
}
```


## Grouping by Specimen

In some cases we've sequenced the same physical specimen multiple times, so we 
combine these replicates for analysis (this way deduplication and such works
across replicates).  Again, we hope to see a high ratio (at least ten sequences
per cell) to imply we've sequenced deeply.

```{r}
with(NULL, {
  if (! is.null(counts_stats$by_amplicon)) {
    tbl <- counts_stats$by_amplicon
    idx <- match(counts_stats$by_amplicon$Specimen, metadata$specimens$Specimen)
    tbl$CellType <- metadata$specimens$CellType[idx]
    tbl <- tbl[, c("Subject", "Timepoint", "ChainType", "CellType", "Specimen", "Seqs", "CellCount", "Ratio")]
    tbl <- tbl[do.call(order, tbl), ]
    drawtab(tbl)
  } else {
    cat("by-amplicon CSV not found")
  }
})

```

### pRESTO processing

pRESTO pairs forward and reverse reads, applies a quality filter, and checks for
the expected primer sequence at the start of the amplicons.  We can check the 
sequence counts as compared with the cell counts again through each step.  (I've
set up SONAR to use the last step's output as input for IgG samples, while 
IgDiscover uses the earlier per-specimen files and does its own read-merging 
before analyzing the IgM samples.)  Since these numbers are not deduplicated in
any way yet, we ideally will not lose many reads relative to the previous table.

```{r}
if (! is.null(counts_stats$by_amplicon)) {
  tbl2 <- subset(counts_presto, select = -c(Subject, Timepoint, Chain))
  idx <- order(match(with(tbl2, paste(Specimen, ChainType)), with(tbl, paste(Specimen, ChainType))))
  drawtab(tbl2[idx, ])
}

```
